# Synthetic Survey Generation Module

This module is a tool for conducting a **simulated user study** by generating synthetic opinion data (survey responses) using Large Language Models (LLMs) like OpenAI's GPT and Google's Gemini.

Its core function is to assess the **perceived relevance and acceptance** of the research findings from the "Python Under the Microscope: A Comparative Energy Analysis of Execution Methods" paper, specifically regarding the **GreenScore metric** and the overall ranking of Python execution models.

## Purpose within the Research

The synthetic data generated by this module serves as a stand-in for real-world professional feedback, allowing for analysis on topics such as:

1.  **GreenScore Acceptance:** Evaluating whether a professional audience finds a unified sustainability metric ($\text{Energy} + \text{Carbon} + \text{Time}$) relevant and effective.
2.  **Runtime Opinion:** Gathering simulated professional consensus on which execution method (`CPython`, `PyPy`, `Cython`, `ctypes`) is considered the most **practical for sustainable large-scale use**.
3.  **Sustainability Awareness:** Modeling the general awareness and importance assigned to software sustainability factors (energy, carbon, performance) by experienced developers.

The LLMs are instructed via the `SURVEY_PROMPT` to adopt the persona of an experienced software professional to ensure the responses are realistic and domain-aware.

## Module Structure

```bash

/synthetic\_survey
├── chatgpt.py             \# Script for generating responses using the OpenAI API (GPT-4o-mini).
├── gemini\_multiple.py     \# Script for generating responses using the Google Gemini API (gemini-2.5-flash).
└── utils.py               \# Shared code: Pydantic Schema, Survey Prompt, and CSV saving function.

````

### File Details

| File | LLM Model | Key Features |
| :--- | :--- | :--- |
| **`chatgpt.py`** | OpenAI (GPT-4o-mini) | Uses explicit system prompt instruction for JSON output and performs **Pydantic validation** post-generation to ensure schema compliance. |
| **`gemini_multiple.py`** | Google (Gemini 2.5 Flash) | Leverages the native **Pydantic schema enforcement** via the `response_schema` configuration parameter for robust structured output. |
| **`utils.py`** | N/A | Defines the **`SurveyResponse` Pydantic schema** (which strictly mirrors the survey form), the specialized **`SURVEY_PROMPT`** (defining the professional persona), and the **`save_to_csv`** utility function. |



## Code Mechanism: How Structured Data is Ensured

The primary challenge in generating reliable synthetic data is ensuring the LLM output is a **perfectly formatted, valid JSON object** that adheres to the required survey structure. This module solves this using a three-part mechanism:

### 1. Pydantic as the Data Contract (`utils.py`)

The **`SurveyResponse`** class in `utils.py` is the single source of truth for the data structure. It uses Python's `Literal` type hints to enforce that responses for multiple-choice questions can *only* be one of the defined options (e.g., "Yes", "No", or specific roles/importance levels). This Pydantic model acts as a contract that both LLMs must honor.

### 2. Prompt-Engineering for Persona and Format (`utils.py`)

The **`SURVEY_PROMPT`** serves two critical functions:
* **Persona Definition:** It forces the LLM to answer from the perspective of an "experienced software professional" with knowledge of "green computing" and "Python optimization." This simulates an informed participant base.
* **Question Context:** It provides the full set of survey questions the LLM must answer in the structured format.

### 3. Distinct API Enforcement Strategies

#### A. OpenAI (GPT) Mechanism (`chatgpt.py`)

1.  **Schema Injection:** The script uses the `SurveyResponse.model_json_schema()` method to generate the *exact JSON schema description* and injects this schema directly into the **system prompt**. This instructs the model on the required keys, types, and constraints.
2.  **JSON Mode:** The API call explicitly sets `response_format={"type": "json_object"}`.
3.  **Post-Validation:** After receiving the raw JSON text from the model, the code runs a crucial validation step: `SurveyResponse.model_validate_json(raw_json)`. If the LLM output contains a typo, a missing field, or an invalid value, this check raises a `ValidationError`, and the response is discarded, ensuring only perfect data is saved.

#### B. Google Gemini Mechanism (`gemini_multiple.py`)

1.  **Native Schema Enforcement:** The Gemini API offers direct support for Pydantic models. The script passes the Pydantic class itself into the configuration: `response_schema=SurveyResponse`.
2.  **Automatic Parsing:** The Gemini client handles the internal prompt-engineering and is designed to return a structured response that can be automatically parsed into the `response.parsed` attribute as a live `SurveyResponse` object. This process is generally more robust and less reliant on explicit prompt instructions for format adherence.

## Getting Started

### Prerequisites

1.  **Python Environment:** Python 3.9+
2.  **Required Libraries:**
    ```bash
    pip install openai google-genai pydantic
    ```
3.  **API Keys:** Ensure your OpenAI or Gemini API keys are accessible, ideally set as environment variables.

### How to Run Generation

Adjust the `NUM_RESPONSES` variable in the script you intend to run (e.g., set to `50`).

#### 1. Generating with OpenAI

```bash
python chatgpt.py
# Output saved to: synthetic_survey_responses_chatgpt.csv
````

#### 2\. Generating with Google Gemini

```bash
python gemini_multiple.py
# Output saved to: synthetic_survey_responses_gemini.csv
```

The scripts will run iteratively, pausing briefly between calls to respect rate limits, and provide real-time feedback on successful data validation and saving.

```
